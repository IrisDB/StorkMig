{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare ACC data for analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information\n",
    "Project: White Stork Affenberg, CareCenter, CASCB  \n",
    "Author: Iris Bontekoe - parts of the code were provided by Andrea Flack (in R)  \n",
    "Date Started: 13 July 2021  \n",
    "Date Version 01: 13 July 2021  \n",
    "Date last modified: 11 October 2021  \n",
    "\n",
    "Program: Python 3.9\n",
    "\n",
    "Description: This script loads the data and prepares it for the analyses. \n",
    "\n",
    "#### Version control\n",
    "01: First version\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "Make sure that the script is executed from top to bottom to ensure that everything works. Once a part of the script is executed and the files are saved, you can run the \"Preparation\" and then continue executing the next part without first executing the previous parts again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff that is necessary to execute the script\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import datetime\n",
    "#import pickle\n",
    "#import time\n",
    "import numpy as np\n",
    "#import geopy.distance\n",
    "#from collections import namedtuple\n",
    "#from astral import LocationInfo\n",
    "#from astral.sun import sun\n",
    "from statistics import mean\n",
    "\n",
    "# Set the path to the folder where the data is located\n",
    "data_folder = \"C:/Users/ibontekoe/Desktop/R_Analyses/DATA/\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For progress bar\n",
    "import time, sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of columns to keep\n",
    "columns_to_keep = [\n",
    "    \"timestamp\",\n",
    "    \"tag-local-identifier\",\n",
    "    \"individual-local-identifier\",\n",
    "    \"eobs:accelerations-raw\",\n",
    "    \"sensor-type\"\n",
    "]\n",
    "\n",
    "# Load the release-death data\n",
    "ReleaseDeath = pd.read_csv(data_folder + \"Release_Death.csv\",sep=\",\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "# Write a function to make sure all objects are removed afterwards\n",
    "def function_prep(Released):\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_csv(data_folder+file_name_in,sep=\",\",low_memory=False)\n",
    "\n",
    "    # Only keep columns that are necessary for further steps in the analyses\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # Only keep ACC data\n",
    "    data = data[data[\"sensor-type\"]==\"acceleration\"]\n",
    "\n",
    "    # Convert the timestamps\n",
    "    data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "    # Remove data before release and after death\n",
    "    \n",
    "    # Add the timing of release and death to the data\n",
    "    data[\"Release\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Release\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "    data[\"Death\"] = pd.to_datetime(data[\"tag-local-identifier\"].map(ReleaseDeath[ReleaseDeath[\"Aviary\"].isin(aviary)].set_index(\"tag.local.identifier\")[\"Death\"].to_dict()),format=\"%d-%m-%Y %H:%M\")\n",
    "\n",
    "    # Only keep data after release and before death\n",
    "    data = data[(data[\"Release\"].isna() | (data[\"timestamp\"] >= data[\"Release\"])) & (data[\"Death\"].isna() | (data[\"timestamp\"] <= data[\"Death\"]))]\n",
    "   \n",
    "    # Sort the data\n",
    "    data.sort_values([\"tag-local-identifier\",\"timestamp\"], ascending=[True,True], inplace=True)\n",
    "\n",
    "    #--- Split the ACC into colums X Y Z ---#\n",
    "    \n",
    "    # Make empty list to store the values in\n",
    "    timestamp = []\n",
    "    identifier = []\n",
    "    X_raw = []\n",
    "    Y_raw = []\n",
    "    Z_raw = []\n",
    "    ACC_num = []\n",
    "    \n",
    "    for ind in data[\"tag-local-identifier\"].unique():\n",
    "        timestamp__ = []\n",
    "        identifier__ = []\n",
    "        X_raw__ = []\n",
    "        Y_raw__ = []\n",
    "        Z_raw__ = []\n",
    "        ACC_num__ = []\n",
    "\n",
    "        data2 = data[data[\"tag-local-identifier\"]==ind]\n",
    "        \n",
    "        # Split the ACC values into columns\n",
    "        for i in range(len(data2)): \n",
    "\n",
    "            # Split the ACC values\n",
    "            SplittedACC = data2.iloc[i][\"eobs:accelerations-raw\"].split()\n",
    "            timestamp_ = np.repeat(data2.iloc[i][\"timestamp\"],len(SplittedACC)/3)\n",
    "            identifier_ = np.repeat(data2.iloc[i][\"tag-local-identifier\"],len(SplittedACC)/3)\n",
    "            X_raw_ = SplittedACC[0:len(SplittedACC):3]\n",
    "            Y_raw_ = SplittedACC[1:len(SplittedACC):3]\n",
    "            Z_raw_ = SplittedACC[2:len(SplittedACC):3]\n",
    "            ACC_num_ = [*range(0+1,round(len(SplittedACC)/3)+1)]\n",
    "\n",
    "            timestamp__ += timestamp_.tolist()\n",
    "            identifier__ += identifier_.tolist()\n",
    "            X_raw__ += X_raw_\n",
    "            Y_raw__ += Y_raw_\n",
    "            Z_raw__ += Z_raw_\n",
    "            ACC_num__ += ACC_num_\n",
    "            \n",
    "        timestamp += timestamp__\n",
    "        identifier += identifier__\n",
    "        X_raw += X_raw__\n",
    "        Y_raw += Y_raw__\n",
    "        Z_raw += Z_raw__\n",
    "        ACC_num += ACC_num__\n",
    "\n",
    "    # Combine the list into a data frame\n",
    "    data = pd.DataFrame({\"timestamp\":timestamp,\"tag-local-identifier\":identifier,\"X_raw\":X_raw,\"Y_raw\":Y_raw,\"Z_raw\":Z_raw,\"ACC_num\": ACC_num})\n",
    "\n",
    "    #--- Transform the values to g or m/s2 ---#\n",
    "    \n",
    "    # Calibrate ACC data # values from Andrea\n",
    "    cal_xzero = 2042\n",
    "    cal_cx = 0.0020\n",
    "    cal_yzero = 2042\n",
    "    cal_cy = 0.0020\n",
    "    cal_zzero = 2049\n",
    "    cal_cz = 0.0023\n",
    "    cal_g = 9.80665\n",
    "\n",
    "    # Convert ACC raw values to meaningful unit (m/s2)\n",
    "    data[\"X_mps\"] = [(int(i)-cal_xzero)*cal_cx*cal_g for i in data[\"X_raw\"]]\n",
    "    data[\"Y_mps\"] = [(int(i)-cal_yzero)*cal_cy*cal_g for i in data[\"Y_raw\"]]\n",
    "    data[\"Z_mps\"] = [(int(i)-cal_zzero)*cal_cz*cal_g for i in data[\"Z_raw\"]]\n",
    "    \n",
    "    # Add the aviary\n",
    "    data[\"Aviary\"] = Aviary\n",
    "    \n",
    "    # Add an individual ID\n",
    "    data[\"Individual\"] = data[\"Aviary\"].map(str)+\"_\"+data[\"tag-local-identifier\"].map(str)\n",
    "    \n",
    "    # Save the data into a new csv or pkl file\n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 10:24:35.630651\n",
      "0:06:30.155377\n"
     ]
    }
   ],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"White Stork Affenberg releases MPIAB_ACC.csv\"\n",
    "file_name_out = \"DataAff_ACC.pkl\"\n",
    "aviary = [\"Affenberg_2019\",\"Affenberg_2020\"]\n",
    "Aviary = \"Affenberg\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_prep(Released=True)\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 10:26:19.160897\n",
      "0:01:43.514324\n"
     ]
    }
   ],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"LifeTrack White Stork SW Germany Care Centre Releases_ACC.csv\"\n",
    "file_name_out = \"DataCC_ACC.pkl\"\n",
    "aviary = [\"CareCenter_2019\",\"CareCenter_2020\"]\n",
    "Aviary = \"CareCenter\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_prep(Released=True)\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 10:30:23.587955\n",
      "0:04:04.416914\n"
     ]
    }
   ],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"LifeTrack White Stork SW Germany CASCB_ACC.csv\"\n",
    "file_name_out = \"DataCASCB_ACC.pkl\"\n",
    "aviary = [\"CASCB_East\",\"CASCB_West\"]\n",
    "Aviary = \"CASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "function_prep(Released=False)\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate ODBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def CalculateODBA():\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_pickle(data_folder+file_name_in)\n",
    "\n",
    "    # Round the timestamp to minutes\n",
    "    data[\"time\"] = data[\"timestamp\"].round('min')\n",
    "\n",
    "    # Make empty list to store the values in\n",
    "    Timestamp = []\n",
    "    Tag = []\n",
    "    ODBA = []\n",
    "    Ind = []\n",
    "\n",
    "    for tsp in data[\"time\"].unique():\n",
    "        \n",
    "        # Make empty list to store the values in\n",
    "        Timestamp_ = []\n",
    "        Tag_ = []\n",
    "        ODBA_ = []\n",
    "        Ind_ = []\n",
    "\n",
    "        data2 = data[data[\"time\"]==tsp]\n",
    "\n",
    "        for ind in data2[\"tag-local-identifier\"].unique():\n",
    "\n",
    "            data3 = data2[data2[\"tag-local-identifier\"]==ind]\n",
    "\n",
    "            # Calculate ODBA\n",
    "            Diff_X = abs(data3[\"X_mps\"] - mean(data3[\"X_mps\"]))\n",
    "            Diff_Y = abs(data3[\"Y_mps\"] - mean(data3[\"Y_mps\"]))\n",
    "            Diff_Z = abs(data3[\"Z_mps\"] - mean(data3[\"Z_mps\"]))\n",
    "\n",
    "            ODBA__ = mean(Diff_X)+mean(Diff_Y)+mean(Diff_Z)\n",
    "\n",
    "            Timestamp__ = data3[\"timestamp\"].unique()[0]\n",
    "            Tag__ = ind\n",
    "            Ind__ = data3[\"Individual\"].unique()[0]\n",
    "\n",
    "            # Put values into list\n",
    "            Timestamp_ += [Timestamp__]\n",
    "            Tag_ += [Tag__]\n",
    "            ODBA_ += [ODBA__]\n",
    "            Ind_ += [Ind__]\n",
    "\n",
    "        # Put values into list\n",
    "        Timestamp += Timestamp_\n",
    "        Tag += Tag_\n",
    "        ODBA += ODBA_\n",
    "        Ind += Ind_     \n",
    "\n",
    "    # Combine the lists into a dataframe\n",
    "    data = pd.DataFrame({\"timestamp\": Timestamp,\"tag-local-identifier\": Tag,\"Individual\": Ind,\"ODBA\": ODBA})\n",
    "\n",
    "    data[\"Aviary\"] = Aviary\n",
    "    data[\"Day\"] = data[\"timestamp\"].dt.date\n",
    "\n",
    "    # Save the data\n",
    "    data.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 11:42:57.853055\n",
      "0:56:14.152247\n"
     ]
    }
   ],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataAff_ACC.pkl\"\n",
    "file_name_out = \"DataAff_ODBA.pkl\"\n",
    "Aviary = \"Affenberg\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalculateODBA()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 12:00:14.915959\n",
      "0:17:17.046102\n"
     ]
    }
   ],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCC_ACC.pkl\"\n",
    "file_name_out = \"DataCC_ODBA.pkl\"\n",
    "Aviary = \"CareCenter\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalculateODBA()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 12:26:43.821850\n",
      "0:26:28.894920\n"
     ]
    }
   ],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in = \"DataCASCB_ACC.pkl\"\n",
    "file_name_out = \"DataCASCB_ODBA.pkl\"\n",
    "Aviary = \"CASCB\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CalculateODBA()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weather variables, the location and flight information\n",
    "\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function nearest\n",
    "def nearest(items, pivot):\n",
    "    return min(items, key=lambda x: abs(x - pivot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def AddVariables():\n",
    "\n",
    "    # Load ODBA data\n",
    "    data_O = pd.read_pickle(data_folder+file_name_in_ODBA)\n",
    "    #data123=data_O\n",
    "\n",
    "    # Load GPS data\n",
    "    data_G = pd.read_pickle(data_folder+file_name_in_GPS)\n",
    "    \n",
    "    # Only keep data during flight (during a burst)\n",
    "    #data_G = data_G[data_G[\"BelongsToBurst\"]]\n",
    "    #data_G = data_G[~data_G[\"FlyingID\"].isna()]\n",
    "\n",
    "    # Remove ACC data that is outside the subset of GPS data\n",
    "    data_G[\"IndDay\"] = data_G[\"tag-local-identifier\"].map(str)+\"_\"+data_G[\"Day\"].map(str)\n",
    "    data_O[\"IndDay\"] = data_O[\"tag-local-identifier\"].map(str)+\"_\"+data_O[\"Day\"].map(str)\n",
    "    data_O = data_O[data_O[\"IndDay\"].isin(data_G[\"IndDay\"])]\n",
    "\n",
    "    # Add a new column to data to enter Burst_A\n",
    "    data_O[\"Burst_A\"] = np.nan\n",
    "    \n",
    "    # Find the nearest start time for every burst and enter the start time in the new column\n",
    "    data_O.reset_index(drop=True, inplace=True)\n",
    "    data_O[\"UniqueNumber\"] = range(len(data_O))\n",
    "\n",
    "    for ind in data_G[\"tag-local-identifier\"].unique():\n",
    "\n",
    "        data_O_sub = data_O[data_O[\"tag-local-identifier\"]==ind]\n",
    "        data_G_sub = data_G[data_G[\"tag-local-identifier\"]==ind]\n",
    "        \n",
    "        for row in data_O_sub[\"UniqueNumber\"]:\n",
    "            First_timestamp = data_O.loc[data_O[\"UniqueNumber\"]==row,\"timestamp\"].unique()\n",
    "            if (First_timestamp < (min(data_G_sub[\"timestamp\"])-datetime.timedelta(minutes=10))):\n",
    "                continue\n",
    "                \n",
    "            if (First_timestamp > (max(data_G_sub[\"timestamp\"])+datetime.timedelta(minutes=10))):\n",
    "                continue\n",
    "\n",
    "            TimeSTPs = data_G_sub[data_G_sub[\"timestamp\"]<=First_timestamp[0]][\"timestamp\"].unique()\n",
    "           \n",
    "            if len(TimeSTPs)<1:\n",
    "                continue\n",
    "                \n",
    "            Nearest = nearest(items=TimeSTPs,pivot=First_timestamp)\n",
    "            Nearest = data_G_sub.loc[data_G_sub[\"timestamp\"]==Nearest,\"Burst_A\"].unique()\n",
    "            \n",
    "            if pd.isnull(Nearest):\n",
    "                continue\n",
    "                        \n",
    "            data_O.loc[data_O[\"UniqueNumber\"]==row,\"Burst_A\"] = Nearest\n",
    "\n",
    "    data_O[\"DiffTime\"]=abs((pd.to_datetime(data_O[\"Burst_A\"]) - pd.to_datetime(data_O[\"timestamp\"])).dt.total_seconds()/60)\n",
    "    \n",
    "\n",
    "    # Replace Burst_A by NA if the timedifference is more than 12 minutes\n",
    "    data_O.loc[~(data_O[\"DiffTime\"].isna()|(data_O[\"DiffTime\"]<12)),\"Burst_A\"] = np.nan\n",
    "\n",
    "    # Remove the data where Burst_A is NA\n",
    "    data_O = data_O[~(data_O[\"Burst_A\"].isna())]\n",
    "\n",
    "    # Make a table with the information from the GPS data\n",
    "    data_G2 = data_G.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\"],as_index=False).agg(\n",
    "        WindSupportPL_Mean = (\"WindSupport_PL\",'mean'),\n",
    "        TempSL_Mean = (\"Temp_SL\",'mean'),\n",
    "        WindSpeedPL_Mean = (\"WindSpeed_PL\",'mean'),\n",
    "        TimestampG_End = (\"timestamp\",'last'),\n",
    "        Lat_End = (\"location-lat\",'last'),\n",
    "        Long_End = (\"location-long\",'last')\n",
    "    )\n",
    "\n",
    "    # Summarise the behaviour (flying, gliding, climbing) and weather at the end (last 5 seconds) of the burst\n",
    "    data_G3 = data_G.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\"],as_index=False).tail(5)\n",
    "    data_G3 = data_G3.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\"],as_index=False).agg(\n",
    "        FlyingID = (\"FlyingID\",'mean'),\n",
    "        GlidingID = (\"GlidingID\",'mean'),\n",
    "        ClimbingID = (\"ClimbingID\",'mean'),\n",
    "        Altitude_End = (\"Altitude\",'mean'),\n",
    "        WindSupportPL_End = (\"WindSupport_PL\",'mean'),\n",
    "        WindSpeedPL_End = (\"WindSpeed_PL\",'mean'),\n",
    "        TempSL_End = (\"Temp_SL\",'mean'),\n",
    "        #TimestampG_End = (\"timestamp\",'last'),\n",
    "        ClimbingRate_End = (\"ClimbingRate\",'mean'),\n",
    "        Sink_End = (\"ClimbingRate\",'mean'),\n",
    "        GlidingSpeed_End = (\"ground-speed\",'mean'),\n",
    "        GlidingAirspeed_End = (\"AirSpeed_PL\",'mean')\n",
    "    )\n",
    "\n",
    "    # Summarise flight properties during climbing bits\n",
    "    data_G4 = data_G[~(data_G[\"ClimbingID\"].isna())]\n",
    "    data_G4 = data_G4.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\"],as_index=False).agg(\n",
    "        ClimbingRate_Mean = (\"ClimbingRate\",'mean')\n",
    "    )\n",
    "\n",
    "    # Summarise flight properties during climbing bits\n",
    "    data_G5 = data_G[~(data_G[\"GlidingID\"].isna())]\n",
    "    data_G5 = data_G5.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\"],as_index=False).agg(\n",
    "        Sink_Mean = (\"ClimbingRate\",'mean'),\n",
    "        GlidingSpeed_Mean = (\"ground-speed\",'mean'),\n",
    "        GlidingAirspeed_Mean = (\"AirSpeed_PL\",'mean')\n",
    "    )\n",
    "\n",
    "    # Summarise the data for the last gliding segment of the burst (only if the gliding segment is at the end)\n",
    "    data_G3g = data_G3[~(data_G3[\"GlidingID\"].isna())]\n",
    "    data_G3g = data_G3g[[\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\",\"GlidingID\"]]\n",
    "    data_G[\"Burst_A\"] = data_G.Burst_A.astype('datetime64[ns]')\n",
    "    data_G3g = pd.merge(data_G,data_G3g)\n",
    "    \n",
    "    data_G3g = data_G3g.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\",\"GlidingID\"],as_index=False).agg(\n",
    "        Sink_Segm = (\"ClimbingRate\",'mean'),\n",
    "        GlidingSpeed_Segm = (\"ground-speed\",'mean'),\n",
    "        GlidingAirspeed_Segm = (\"AirSpeed_PL\",'mean'),\n",
    "        WindSupportPL_Segm = (\"WindSupport_PL\",'mean')\n",
    "    )\n",
    "    \n",
    "    # Summarise the data for the last climbing segment of the burst (only if the climbing segment is at the end)\n",
    "    data_G3c = data_G3[~(data_G3[\"ClimbingID\"].isna())]\n",
    "    data_G3c = data_G3c[[\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\",\"ClimbingID\"]]\n",
    "    data_G3c = pd.merge(data_G,data_G3c)\n",
    "    \n",
    "    data_G3c = data_G3c.groupby([\"Aviary\",\"Day\",\"tag-local-identifier\",\"Individual\",\"BurstID\",\"Burst_A\",\"IndDay\",\"ClimbingID\"],as_index=False).agg(\n",
    "        ClimbingRate_Segm = (\"ClimbingRate\",'mean')\n",
    "    )\n",
    "\n",
    "    # Add the information to the ODBA data\n",
    "    data_O[\"Burst_A\"] = data_O.Burst_A.astype('datetime64[ns]')\n",
    "    #data_G2[\"Burst_A\"] = data_G2.Burst_A.astype('datetime64[ns]')\n",
    "    #data_O.Day.astype('datetime64[ns]')\n",
    "    #data_G2.Day.astype('datetime64[ns]')\n",
    "\n",
    "    data_O2 = pd.merge(data_O,data_G2,on=[\"IndDay\",\"Burst_A\",\"tag-local-identifier\",\"Aviary\",\"Day\",\"Individual\"],how=\"outer\")\n",
    "\n",
    "    data_O2 = pd.merge(data_O2,data_G3,on=[\"IndDay\",\"Burst_A\",\"tag-local-identifier\",\"Aviary\",\"Day\",\"Individual\",\"BurstID\"],how=\"outer\")\n",
    "    data_O2 = pd.merge(data_O2,data_G4,on=[\"IndDay\",\"Burst_A\",\"tag-local-identifier\",\"Aviary\",\"Day\",\"Individual\",\"BurstID\"],how=\"outer\")\n",
    "    data_O2 = pd.merge(data_O2,data_G5,on=[\"IndDay\",\"Burst_A\",\"tag-local-identifier\",\"Aviary\",\"Day\",\"Individual\",\"BurstID\"],how=\"outer\")\n",
    "    data_O2 = pd.merge(data_O2,data_G3g,on=[\"IndDay\",\"Burst_A\",\"tag-local-identifier\",\"Aviary\",\"Day\",\"Individual\",\"BurstID\",\"GlidingID\"],how=\"outer\")\n",
    "    data_O2 = pd.merge(data_O2,data_G3c,on=[\"IndDay\",\"Burst_A\",\"tag-local-identifier\",\"Aviary\",\"Day\",\"Individual\",\"BurstID\",\"ClimbingID\"],how=\"outer\")\n",
    "\n",
    "    data_O2[\"DiffTime_\"] = (data_O2[\"timestamp\"] - data_O2[\"TimestampG_End\"]).dt.total_seconds()\n",
    "\n",
    "    # Remove data that does not fit to a burst\n",
    "    data_O2 = data_O2[~(data_O2[\"DiffTime_\"].isna())]\n",
    "    data_O2 = data_O2[data_O2[\"DiffTime_\"]>=0]\n",
    "    data_O2 = data_O2[data_O2[\"DiffTime_\"]<120]\n",
    "    \n",
    "    # Add a column for climbing vs gliding\n",
    "    data_O2[\"FlightType\"] = np.nan\n",
    "    data_O2.loc[~(data_O2[\"GlidingID\"].isna()),\"FlightType\"] = \"Gliding\"\n",
    "    data_O2.loc[~(data_O2[\"ClimbingID\"].isna()),\"FlightType\"] = \"Climbing\"\n",
    "\n",
    "    #-----------------#\n",
    "    #- Save the data -#\n",
    "    #-----------------#\n",
    "    # Save the data into a new pkl file\n",
    "    data_O2.to_pickle(data_folder+file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-18 16:11:06.387350\n",
      "0:04:05.148521\n"
     ]
    }
   ],
   "source": [
    "#=================#\n",
    "#=== Affenberg ===#\n",
    "#=================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in_ODBA = \"DataAff_ODBA.pkl\"\n",
    "file_name_in_GPS = \"DataAff_TempWind_S.pkl\"\n",
    "file_name_out = \"DataAff_ODBA_Temp.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "AddVariables()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-18 16:29:49.350055\n",
      "0:03:35.696099\n"
     ]
    }
   ],
   "source": [
    "#==================#\n",
    "#=== CareCenter ===#\n",
    "#==================#\n",
    "\n",
    "# Define objects\n",
    "file_name_in_ODBA = \"DataCC_ODBA.pkl\"\n",
    "file_name_in_GPS = \"DataCC_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCC_ODBA_Temp.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "AddVariables()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-18 16:42:34.271731\n",
      "0:12:44.889233\n"
     ]
    }
   ],
   "source": [
    "#=============#\n",
    "#=== CASCB ===#\n",
    "#=============#\n",
    "\n",
    "# Define objects\n",
    "file_name_in_ODBA = \"DataCASCB_ODBA.pkl\"\n",
    "file_name_in_GPS = \"DataCASCB_TempWind_S.pkl\"\n",
    "file_name_out = \"DataCASCB_ODBA_Temp.pkl\"\n",
    "\n",
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "AddVariables()\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for the different studies\n",
    "def CombineAll(pattern):\n",
    "\n",
    "    # Find all file names\n",
    "    all_files = glob.glob(data_folder+\"Data\"+\"*\"+pattern+\".pkl\")\n",
    "\n",
    "    # Load all data files for Affenberg\n",
    "    data_all = (pd.read_pickle(f) for f in all_files)\n",
    "\n",
    "    # Merge the files together\n",
    "    data = pd.concat(data_all)\n",
    "\n",
    "    # Save the data into a new csv file\n",
    "    data.to_csv(data_folder+\"All\"+pattern+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-18 16:42:34.467889\n",
      "0:00:00.162852\n"
     ]
    }
   ],
   "source": [
    "# Execute the function and print the run time\n",
    "start = datetime.datetime.now()\n",
    "CombineAll(pattern=\"_ODBA_Temp\")\n",
    "print(datetime.datetime.now())\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
